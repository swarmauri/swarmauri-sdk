# Peagen Improvement Proposal — **IP-0004**

*EvaluatorPools & `peagen eval` Command*

---

## 1 Overview

IP-0004 adds a first-class **evaluation stage** to the Peagen tool-chain.
A new CLI command, **`peagen eval`**, takes the workspace created by `peagen program fetch`, spins up an **EvaluatorPool** (a thread-safe collection of `EvaluatorBase` evaluators), runs every requested benchmark, and writes an *evaluation manifest* that other tools—or CI—can parse.

---

## 2 Motivation

| Gap                                                 | Consequence                                                                |
| --------------------------------------------------- | -------------------------------------------------------------------------- |
| No integrated evaluation step                       | Teams invent ad-hoc notebooks; results are not comparable across projects. |
| Evaluator abstractions exist but lack a CLI surface | Code rot & duplicated logic.                                               |
| CI needs a simple pass/fail gate                    | Pipelines currently shell-script around `program fetch`.                   |

---

## 3 Command-line Surface

```console
peagen eval WORKSPACE_URI  [PROGRAM_GLOB]  [OPTIONS]

Positional arguments
  WORKSPACE_URI   Local path **or** adapter URI produced by `peagen program fetch`.
  PROGRAM_GLOB    Optional Unix-style glob limiting which programmes to evaluate
                  (defaults to **/*.prog and projects/*/program.yaml).

Options
  -p, --pool TEXT          Dotted path *or* entry-point name of an EvaluatorPool.
                           If omitted, Peagen looks in .peagen.toml → [evaluation].
  -c, --config FILE        YAML/TOML overrides merged into the pool definition.
  --max-workers INTEGER    Upper bound on concurrent evaluator threads (default 10).
  --async / --no-async     Run pool with asyncio instead of threads (default threads).
  --out URI                Where to write the _eval_manifest (default WORKSPACE/.peagen).
  --json                   Emit final manifest to stdout instead of file.
  --strict                 Non-zero exit if **any** evaluator returns 0.0 or raises.
  --skip-failed            Ignore programmes that already failed build/test.
```

### Behaviour Notes

* If `WORKSPACE_URI` is a remote adapter URI, artefacts are first fetched to a temp directory (re-using `peagen.cli_common.temp_workspace` helpers).
* All options **override** values that may be present in `.peagen.toml`.
* The exit code matrix is:

  * `0` – success,
  * `1` – schema-validation failure,
  * `2` – unhandled evaluator or pool exception,
  * `3` – `--strict` threshold breached.

---

## 4 Configuring Evaluator Pools in `.peagen.toml`

Peagen already discovers plugins through *entry-points* via **`plugin_registry`**.
Evaluator pools and individual evaluators therefore need only be referenced in the TOML; no “preset registry” code is added.

```toml
# ── NEW SECTION ─────────────────────────────────────────
[evaluation]
 pool        = "swarmauri_standard.evaluator_pools.EvaluatorPool.EvaluatorPool"
              # Dotted or "module:Class" path, or entry-point name.
max_workers = 16          # Overrides CLI default.
async       = false       # true = asyncio pool.
strict      = true        # Fail the run if any score == 0.

[evaluation.evaluators]
# Key names = evaluator IDs that will appear in the manifest
# Each subsection MUST include a class reference and MAY include kwargs.

[evaluation.evaluators.bleu]
cls  = "my_pkg.metrics.BLEUEvaluator"
ngram = 4                 # Arbitrary kwargs passed to __init__

[evaluation.evaluators.rouge]
cls  = "my_pkg.metrics.ROUGEEvaluator"

[evaluation.evaluators.latency]
cls  = "peagen.metrics.LatencyEvaluator"
percentile = 95
```

### Resolution Order

1. CLI flags (`--pool`, `--config`, etc.).
2. `.peagen.toml` → `[evaluation]`.
3. Built-in default pool (`swarmauri_standard.evaluator_pools.EvaluatorPool.EvaluatorPool`).

If the same key appears multiple times the **earlier** entry wins, ensuring CLI always overrides file settings.

---

## 5 Schema Updates

### 5.1 `.peagen.toml.schema` additions

* **New object** `evaluation` (optional).
* Required keys inside `evaluation`:

  * `pool` – string, dotted path or entry-point name.
* Optional keys: `max_workers` (int ≥ 1), `async` (bool), `strict` (bool).
* **Sub-object** `evaluation.evaluators` with pattern-property `^[A-Za-z0-9_-]+$`.

  * Each property value is an object **requiring** `cls` (string) and **optionally** any additional arguments (free-form).
* All strings continue to allow environment-variable interpolation `${FOO}` exactly as other TOML fields do.

### 5.2 `eval_manifest.schema.v1.json` (new)

| Field            | Type            | Constraints                                                     |                  |                  |                |
| ---------------- | --------------- | --------------------------------------------------------------- | ---------------- | ---------------- | -------------- |
| `schemaVersion`  | const `"1.0.0"` | Camel-case (aligns with IP-0003).                               |                  |                  |                |
| `peagen_version` | string          | Regex \`^(?:0                                                   | \[1-9]\d\*).(?:0 | \[1-9]\d\*).(?:0 | \[1-9]\d\*)$\` |
| `workspace_slug` | string          | Slug generated by `peagen program fetch`.                       |                  |                  |                |
| `generated_at`   | string          | ISO-8601 UTC timestamp.                                         |                  |                  |                |
| `pool`           | string          | Fully-qualified class or entry-point of the pool actually used. |                  |                  |                |
| `evaluators`     | array \<string> | Ordered list of evaluator IDs.                                  |                  |                  |                |
| `results`        | array \<Result> | At least one element.                                           |                  |                  |                |

**`Result` object**

| Field          | Type   | Rules                                                     |
| -------------- | ------ | --------------------------------------------------------- |
| `program_path` | string | Relative path from workspace root.                        |
| `scores`       | object | Keys ⊆ `evaluators`, values 0 ≤ *float* ≤ 1 .\*           |
| `metadata`     | object | Arbitrary per-evaluator details — nested objects allowed. |

\* Evaluators that do not emit a numeric score MUST still create an entry with `null`.

### 5.3 Other Schemas

* Add new entry-point group **`evaluator_pools`** to `plugin_registry.GROUPS` if you wish to distribute pools as plugins; otherwise users can always reference dotted paths directly.
* No changes are required to the existing generation or manifest schemas.

---

## 6 Implementation Plan (Core Team)

| Step             | File / Component                                                      | Description                                       |
| ---------------- | --------------------------------------------------------------------- | ------------------------------------------------- |
| CLI command      | `peagen/cli/eval.py`                                                  | Parse args, merge config, pass to core helper.    |
| Workspace loader | `program.py` / `cli_common.temp_workspace`                            | Re-use fetch logic when URI is remote.            |
| Default Pool     | `peagen/eval/pools/default.py`                                        | Threaded pool honouring `max_workers` & `async`.  |
| Manifest writer  | `manifest_writer.py`                                                  | Add `mode="eval"` + JSONL roll-up.                |
| Schema files     | `schemas/eval_manifest.schema.v1.json`, update `peagen.toml.schema.*` | Author & unit-test with `ajv`.                    |
| Tests            | `tests/eval/test_eval_cmd.py`                                         | Functional coverage incl. strict-mode exit codes. |
| Docs             | `docs/cli/eval.md`                                                    | End-to-end guide, CI snippet.                     |

*All tasks are owned by the **Core team**.*

---

## 7 Acceptance Criteria

1. `peagen eval ./sample_ws` writes a manifest that passes schema validation.
2. `--max-workers` limits concurrency; `--async` switches to asyncio.
3. Pool & evaluators declared in `.peagen.toml` are resolved through `plugin_registry`.
4. `--strict` causes exit 3 if any score is 0 or an evaluator raises.
5. Event-bus emits `evaluation.started` and `evaluation.finished` with correct metadata.

---

## 8 Backwards Compatibility

* The new `[evaluation]` section in `.peagen.toml` is **optional**; existing workflows are unaffected.
* Eval manifests live alongside build manifests; downstream tools ignore them unless subscribed.

---

*Prepared by: **Core Team***
*Date: 2025-05-23*
