"""
gateway.api.hooks.worker
────────────────────────
AutoAPI-native hooks for Worker CRUD plus two custom RPCs:

• Workers.heartbeat   – keep-alive
• Work.finished       – task completion report
"""

from __future__ import annotations
import json, time, uuid, httpx
from typing import Any, Dict

from autoapi.v2 import Phase, AutoAPI
from peagen.transport.jsonrpc import RPCException
from peagen.orm import Status, Worker

from .. import (
    WORKER_KEY, WORKER_TTL, READY_QUEUE, TASK_TTL,
    _upsert_worker, _publish_event, _publish_queue_length,
    _save_task, 
    _publish_task,
    _load_task, _finalize_parent_tasks,
    log, queue,
)
from .  import api

# ─────────── get autogenerated schemas once ───────────────────────────
WorkerCreate = AutoAPI.get_schema(Worker, "create")
WorkerRead   = AutoAPI.get_schema(Worker, "read")
WorkerListIn = AutoAPI.get_schema(Worker, "list")


# ──────────────────── 1. Worker CREATE hooks ──────────────────────────
@api.hook(Phase.PRE_TX_BEGIN, method="workers.create")
async def pre_worker_create(ctx: Dict[str, Any]) -> None:
    wc: WorkerCreate = ctx["env"].params

    # Resolve handler list if caller omitted it
    handler_list = wc.handlers or []
    if not handler_list:
        well_known = wc.url.replace("/rpc", "/well-known")
        try:
            async with httpx.AsyncClient(timeout=5) as cl:
                r = await cl.get(well_known)
                if r.status_code == 200:
                    handler_list = r.json().get("handlers", [])
        except Exception as exc:            # noqa: BLE001
            log.warning("well-known fetch for %s failed – %s", wc.url, exc)

    if not handler_list:
        raise RPCException(code=-32602, message="worker supports no handlers")

    # make the validated, updated model available to post-hook
    ctx["worker_in"] = wc.model_copy(update={"handlers": handler_list})


@api.hook(Phase.POST_COMMIT, method="workers.create")
async def post_worker_create(ctx: Dict[str, Any]) -> None:
    created: WorkerRead = ctx["result"]     # row persisted by AutoAPI
    wc: WorkerCreate    = ctx["worker_in"]

    await _upsert_worker(
        created.id,
        {
            "pool":       wc.pool,
            "url":        wc.url,
            "advertises": wc.advertises or {},
            "handlers":   wc.handlers,
        },
    )
    await queue.sadd(f"pool:{wc.pool}:members", created.id)
    log.info("worker %s joined pool %s", created.id, wc.pool)


# ──────────────────── 2. Worker LIST post-hook ────────────────────────
@api.hook(Phase.POST_HANDLER, method="workers.list")
async def post_workers_list(ctx: Dict[str, Any]) -> None:
    """Replace the DB-only result with Redis-filtered live workers."""
    params  = ctx["env"].params
    pool    = params.pool
    keys    = await queue.keys("worker:*")
    now     = int(time.time())
    workers = []

    for k in keys:
        w = await queue.hgetall(k)
        if not w:
            continue
        if now - int(w.get("last_seen", 0)) > WORKER_TTL:
            continue
        if pool and w.get("pool") != pool:
            continue

        advert = json.loads(w["advertises"]) if isinstance(w.get("advertises"), str) else w.get("advertises", {})
        handlers = json.loads(w["handlers"]) if isinstance(w.get("handlers"), str) else w.get("handlers", [])

        workers.append({
            "id": k.split(":", 1)[1],
            "pool": w.get("pool"),
            "url":  w.get("url"),
            "advertises": advert,
            "handlers": handlers,
            "last_seen": int(w["last_seen"]),
        })

    ctx["result"] = workers                      # simple list[dict]


# ──────────────────── 3. Custom RPC: Worker.heartbeat ────────────────
def _heartbeat(env: Dict[str, Any], db):         # db unused
    p = env["params"]                            # raw dict
    wid = p.get("workerId")
    if not queue.exists.sync(WORKER_KEY.format(wid)):
        # allow on-the-fly registration if pool+url supplied
        if "pool" not in p or "url" not in p:
            log.warning("heartbeat from unknown worker %s ignored", wid)
            return {"ok": False}
    mapping = {"last_seen": int(time.time())}
    if p.get("pool"): mapping["pool"] = p["pool"]
    if p.get("url"):  mapping["url"]  = p["url"]
    queue.hset.sync(WORKER_KEY.format(wid), mapping=mapping)
    queue.expire.sync(WORKER_KEY.format(wid), WORKER_TTL)
    return {"ok": True}

api.rpc["Workers.heartbeat"] = _heartbeat




# ─────────────────────────── Workers ────────────────────────────
# workers are stored as hashes:  queue.hset worker:<id> pool url advertises last_seen
WORKER_KEY = "worker:{}"  # format with workerId
WORKER_TTL = 15  # seconds before a worker is considered dead
TASK_TTL = 24 * 3600  # 24 h, adjust as needed


# ─────────────────────────── IP tracking ─────────────────────────

async def _upsert_worker(workerId: str, data: dict) -> None:
    """
    Persist worker metadata in Redis hash `worker:<id>`.
    • Any value that isn't bytes/str/int/float is json-encoded.
    • last_seen is stored as Unix epoch seconds.
    """
    key = WORKER_KEY.format(workerId)
    existing = await queue.hgetall(key)
    for field in ("advertises", "handlers"):
        if field not in data and field in existing:
            try:
                data[field] = json.loads(existing[field])
            except Exception:  # noqa: BLE001
                data[field] = existing[field]

    coerced = {}
    for k, v in data.items():
        if isinstance(v, (bytes, str, int, float)):
            coerced[k] = v
        else:
            coerced[k] = json.dumps(v)  # serialize nested dicts, lists, etc.

    coerced["last_seen"] = int(time.time())  # heartbeat timestamp
    await queue.hset(key, mapping=coerced)
    await queue.expire(key, WORKER_TTL)  # <<—— TTL refresh
    # ensure the worker's pool is tracked so WebSocket clients
    # receive queue length updates even before a task is submitted
    pool = data.get("pool")
    if pool:
        await queue.sadd("pools", pool)
    await _publish_event("worker.update", {"id": workerId, **data})


async def _publish_queue_length(pool: str) -> None:
    qlen = len(await queue.lrange(f"{READY_QUEUE}:{pool}", 0, -1))
    await _publish_event("queue.update", {"pool": pool, "length": qlen})


async def _remove_worker(workerId: str) -> None:
    """Expire the worker's registry entry immediately."""
    await queue.expire(WORKER_KEY.format(workerId), 0)
    await _publish_event("worker.update", {"id": workerId, "removed": True})


async def _live_workers_by_pool(pool: str) -> list[dict]:
    keys = await queue.keys("worker:*")
    workers = []
    now = int(time.time())
    for k in keys:
        w = await queue.hgetall(k)
        if not w:  # TTL expired or never registered
            continue
        if now - int(w["last_seen"]) > WORKER_TTL:
            continue  # stale
        if w["pool"] == pool:
            wid = k.split(":", 1)[1]
            workers.append({"id": wid, **w})
    return workers


def _pick_worker(workers: list[dict], action: str | None) -> dict | None:
    """Return the first worker that advertises *action*."""
    if action is None:
        return None
    for w in workers:
        raw = w.get("handlers", [])
        if isinstance(raw, str):
            try:
                raw = json.loads(raw)
            except Exception:  # noqa: BLE001
                raw = []
        if action in raw:
            return w
    return None
