"""
peagen.handlers.analysis_handler
────────────────────────────────
Run-directory analysis executed by a worker.

* Accepts a validated TaskRead model (generated by AutoAPI).
* Returns a plain `dict` that becomes the Work / Task result payload.
* No import from peagen.transport.jsonrpc_schemas.*.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Optional, Tuple

from autoapi.v2        import AutoAPI
from peagen.orm        import TaskModel            # your ORM class

from peagen._utils              import maybe_clone_repo
from peagen._utils.config_loader import resolve_cfg
from peagen.core.analysis_core   import analyze_runs
from peagen.plugins              import PluginManager
from peagen.plugins.vcs          import pea_ref

from .repo_utils import fetch_repo, cleanup_repo


# ─────────────────────────── AutoAPI schemas ──────────────────────────
TaskRead = AutoAPI.get_schema(TaskModel, "read")        # for incoming payload


# ─────────────────────────── helper fns ───────────────────────────────
def _checkout_repo(repo: Optional[str], ref: str) -> Tuple[Optional[Path], Optional[str]]:
    """Return (tmp_dir, prev_cwd) if *repo* is provided, else (None, None)."""
    if not repo:
        return None, None
    return fetch_repo(repo, ref)     # ⇢ repo_utils.fetch_repo


# ─────────────────────────── main handler  ────────────────────────────
async def analysis_handler(task: TaskRead) -> Dict[str, Any]:
    """
    Parameters
    ----------
    task : TaskRead
        The Task row (validated by AutoAPI) whose `payload.action == "analysis"`.

    Returns
    -------
    dict
        A JSON-serialisable result block stored back on the Task / Work row.
    """
    payload = task.payload or {}
    args: Dict[str, Any] = payload.get("args", {})

    repo: Optional[str] = args.get("repo")
    ref: str            = args.get("ref", "HEAD")
    run_dirs            = [Path(p) for p in args.get("run_dirs", [])]
    spec_name: str      = args.get("spec_name", "analysis")

    # --- clone / checkout repo (if any) -------------------------------
    tmp_dir, prev_cwd = _checkout_repo(repo, ref)

    # --- perform the analysis ----------------------------------------
    with maybe_clone_repo(repo, ref):
        result: Dict[str, Any] = analyze_runs(run_dirs, spec_name=spec_name)

    # --- optional VCS integration ------------------------------------
    cfg = resolve_cfg()
    pm  = PluginManager(cfg)
    try:
        vcs = pm.get("vcs")
    except Exception:          # pragma: no cover – plugin optional
        vcs = None

    if vcs:
        analysis_branch = pea_ref("analysis", spec_name)
        vcs.create_branch(analysis_branch, "HEAD", checkout=True)

        # Ensure runs are merged (ours strategy)
        for rd in run_dirs:
            run_branch = pea_ref("run", rd.name)
            vcs.merge_ours(run_branch, f"merge {run_branch}")

        # Commit results file if present
        if rf := result.get("results_file"):
            repo_root = Path(vcs.repo.working_tree_dir)
            rel_path  = Path(rf).resolve().relative_to(repo_root)
            commit_sha = vcs.commit([str(rel_path)], f"analysis {spec_name}")
            result["commit"] = commit_sha

        vcs.switch("HEAD")
        vcs.push(analysis_branch)
        result["analysis_branch"] = analysis_branch

    # --- clean-up -----------------------------------------------------
    if repo:
        cleanup_repo(tmp_dir, prev_cwd)

    return result
